
@misc{pacheco_integrating_nodate,
	title = {Integrating advanced discrete choice models in mixed integer linear optimization},
	abstract = {The integration of customer behavioral models in Operations Research (OR) is appealing to

operators and policy makers (the supply) because it provides a better understanding of the prefer-
ences of clients (the demand) while planning for their systems. On the one hand, these preferences

are formalized with discrete choice models, which are the state-of-the-art for the mathematical
modeling of demand. On the other hand, the optimization models that are considered to design

and configure a system are associated with Mixed Integer Linear Problems (MILP). Notwith-
standing the clear advantages of this integration, the complexity of discrete choice models leads

to mathematical formulations that are highly nonlinear and nonconvex in the variables of interest,
and therefore difficult to be included in MILP. In this paper, we present a general framework that
overcomes these limitations by integrating in MILP advanced discrete choice models for which

it is possible to draw from the associated probability distribution. We illustrate a concrete appli-
cation where the framework can be employed: benefit maximization from an operator that sells

services to a market at a certain price and with a given capacity. A case study from the recent

literature is considered to perform different experiments on this application, such as price differ-
entiation by population segmentation and benefit maximization through capacity allocation. The

results show that this approach is a powerful tool to configure systems based on the heterogeneous
behavior of customers, and it allows to investigate advanced marketing strategies and business
models.},
	author = {Pacheco, Meritxell and Azadeh, Shadi Sharif and Bierlaire, Michel and Gendron, Bernard},
	keywords = {interview},
	file = {PacShaBieGen20150609.pdf:/home/gayouf/Zotero/storage/QYZ4IZ2J/PacShaBieGen20150609.pdf:application/pdf}
}

@article{zheng_time-dependent_2016,
	title = {Time-dependent area-based pricing for multimodal systems with heterogeneous users in an agent-based environment},
	volume = {62},
	issn = {0968-090X},
	url = {http://www.sciencedirect.com/science/article/pii/S0968090X15003745},
	doi = {10.1016/j.trc.2015.10.015},
	abstract = {In this paper, we investigate an area-based pricing scheme for congested multimodal urban networks with the consideration of user heterogeneity. We propose a time-dependent pricing scheme where the tolls are iteratively adjusted through a Proportional–Integral type feedback controller, based on the level of vehicular traffic congestion and traveler’s behavioral adaptation to the cost of pricing. The level of congestion is described at the network level by a Macroscopic Fundamental Diagram, which has been recently applied to develop network-level traffic management strategies. Within this dynamic congestion pricing scheme, we differentiate two groups of users with respect to their value-of-time (which related to income levels). We then integrate incentives, such as improving public transport services or return part of the toll to some users, to motivate mode shift and increase the efficiency of pricing and to attain equitable savings for all users. A case study of a medium size network is carried out using an agent-based simulator. The developed pricing scheme demonstrates high efficiency in congestion reduction. Comparing to pricing schemes that utilize similar control mechanisms in literature which do not treat the adaptivity of users, the proposed pricing scheme shows higher flexibility in toll adjustment and a smooth behavioral stabilization in long-term operation. Significant differences in behavioral responses are found between the two user groups, highlighting the importance of equity treatment in the design of congestion pricing schemes. By integrating incentive programs for public transport using the collected toll revenue, more efficient pricing strategies can be developed where savings in travel time outweigh the cost of pricing, achieving substantial welfare gain.},
	number = {Supplement C},
	urldate = {2017-10-04},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Zheng, Nan and Rérat, Guillaume and Geroliminis, Nikolas},
	month = jan,
	year = {2016},
	keywords = {interview, Agent-based, Congestion pricing, Macroscopic Fundamental Diagram, Multimodal, User adaptation, User heterogeneity},
	pages = {133--148},
	file = {ScienceDirect Full Text PDF:/home/gayouf/Zotero/storage/2UBQ4NNL/Zheng et al. - 2016 - Time-dependent area-based pricing for multimodal s.pdf:application/pdf;ScienceDirect Snapshot:/home/gayouf/Zotero/storage/2PNJS47V/S0968090X15003745.html:text/html}
}

@misc{he_alternative_nodate,
	title = {Alternative activity pattern generation for stated preference survey},
	abstract = {We present a systematic method for generating activity-driven, multi-day alternative activity patterns that form choice sets for stated preference surveys. An activity pattern consists of information about an individual’s activity agenda, travel modes between activity episodes, and the location and duration of each episode. The proposed method adjusts an individual’s observed activity pattern using a hill-climbing algorithm, an iterative algorithm that finds local optima, to search for the best response to hypothetical system changes. The multi-day approach allows for flexibility to reschedule activities on different days and thus presents a more complete view of demand for activity participation, as these demands are rarely confined to a single day in reality. As a proof-of-concept, we apply the method to a multi-day activity-travel survey in Singapore and consider the hypothetical implementation of an on-demand AV service. The demonstration shows promising results with the algorithm exhibiting overall desirable behavior with reasonable responses. In addition to representing the individual’s direct response, the use of observed patterns also reveals the propagation of impacts, i.e. indirect effects, across the multi-day activity pattern.},
	author = {He, He and Atasoy, Bilge and Brazier, J. Cressica and Zegras, P. Christopher and Ben-Akiva, Moshe},
	keywords = {Students Project, Nicola Ortelli},
	file = {fms-sp_activitypatterngeneration.pdf:/home/gayouf/Zotero/storage/KZJWI8J4/fms-sp_activitypatterngeneration.pdf:application/pdf}
}

@misc{danaf_context-aware_nodate,
	title = {Context-aware stated preferences surveys for smart mobility,},
	abstract = {This paper presents FMS-SP, a web-based tool that utilizes random experimental design in order to collect context-aware stated preferences (SP) data for travel behavior. FMS-SP is part of the Future Mobility Sensing (FMS) platform which is primarily used to collect activity diary data. Leveraging on the individual specific observed travel patterns, FMS-SP generates a daily stated preference question on the choice under different hypothetical scenarios for a realized trip, which ensures context awareness. It includes a wide range of alternatives such as walking, biking, bike-sharing, drive alone, carpooling, car-sharing, taxi, on-demand services, and transit. In addition, it is flexible in terms of introducing new alternative mobility services and different potential operational attributes. FMS-SP profiles are automatically validated to reduce the number of dominant or inferior alternatives in real-time, then validated using Monte-Carlo simulations offline. The implementation and results for two applications of FMS-SP analyzing smart mobility solutions that do not exist in real life are also presented: an on-demand paratransit service, Flexible Mobility on Demand (FMOD) and an app-based travel advisor which incentivizes users to switch towards more environmental friendly alternatives (Tripod).},
	author = {Danaf, Mazen and Ding-Mastera, Jing and Atasoy, Bilge and Lima de Azevedo, Carlos and Abou-Zeid, Maya and Cox, Nathaniel and Zhao, Fang and Ben-Akiva, Moshe},
	keywords = {Students Project, Nicola Ortelli},
	file = {FMS-SP.pdf:/home/gayouf/Zotero/storage/X9LRV457/FMS-SP.pdf:application/pdf}
}

@article{friedman_stochastic_2002,
	series = {Nonlinear {Methods} and {Data} {Mining}},
	title = {Stochastic gradient boosting},
	volume = {38},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947301000652},
	doi = {10.1016/S0167-9473(01)00065-2},
	abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current “pseudo”-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
	number = {4},
	urldate = {2017-10-04},
	journal = {Computational Statistics \& Data Analysis},
	author = {Friedman, Jerome H.},
	month = feb,
	year = {2002},
	keywords = {Thesis},
	pages = {367--378},
	file = {ScienceDirect Full Text PDF:/home/gayouf/Zotero/storage/UUL82EZV/Friedman - 2002 - Stochastic gradient boosting.pdf:application/pdf;ScienceDirect Snapshot:/home/gayouf/Zotero/storage/GDXRQYX7/S0167947301000652.html:text/html}
}

@inproceedings{paredes_machine_2017,
	title = {Machine learning or discrete choice models for car ownership demand estimation and prediction?},
	doi = {10.1109/MTITS.2017.8005618},
	abstract = {Discrete choice models are widely used to explain transportation behaviors, including a household's decision to own a car. They show how some distinct choice of human behavior or preference influences a decision. They are also used to project future demand estimates to support policy exploration. This latter use for prediction is indirectly aligned with and conditional to the model's estimation which aims to fit the observed data. In contrast, machine learning models are derived to maximize prediction accuracy through mechanisms such as out-of-sample validation, non-linear structure, and automated covariate selection, albeit at the expense of interpretability and sound behavioral theory. We investigate how machine learning models can outperform discrete choice models for prediction of car ownership using transportation household survey data from Singapore. We compare our household car ownership model (multinomial logit model) against various machine learning models (e.g. Random Forest, Support Vector Machines) by using 2008 data to derive, i.e. estimate models that we then use to predict 2012 ownership. The machine learning models are inferior to the discrete choice model when using discrete choice features. However, after engineering features more appropriate for machine learning they are superior. These results highlight both the cost of applying machine learning models in econometric contexts and an opportunity for improved prediction and better urban policy making through machine learning models with appropriate features.},
	booktitle = {2017 5th {IEEE} {International} {Conference} on {Models} and {Technologies} for {Intelligent} {Transportation} {Systems} ({MT}-{ITS})},
	author = {Paredes, M. and Hemberg, E. and O'Reilly, U. M. and Zegras, C.},
	month = jun,
	year = {2017},
	keywords = {Biological system modeling, Data models, Thesis, Automobiles, car ownership, discrete choice models, econometrics, Estimation, learning (artificial intelligence), machine learning models, Predictive models, Support vector machines, transportation, transportation behaviors, transportation household survey data},
	pages = {780--785},
	file = {IEEE Xplore Abstract Record:/home/gayouf/Zotero/storage/KP42ML2A/8005618.html:text/html;IEEE Xplore Full Text PDF:/home/gayouf/Zotero/storage/JHUMAZ8D/Paredes et al. - 2017 - Machine learning or discrete choice models for car.pdf:application/pdf}
}

@article{domingos_few_2012,
	title = {A {Few} {Useful} {Things} to {Know} {About} {Machine} {Learning}},
	volume = {55},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2347736.2347755},
	doi = {10.1145/2347736.2347755},
	abstract = {Tapping into the "folk knowledge" needed to advance machine learning applications.},
	number = {10},
	urldate = {2017-10-15},
	journal = {Commun. ACM},
	author = {Domingos, Pedro},
	month = oct,
	year = {2012},
	pages = {78--87},
	file = {cacm12.pdf:/home/gayouf/Zotero/storage/ZYWXBICL/cacm12.pdf:application/pdf}
}

@article{hagenauer_comparative_2017,
	title = {A comparative study of machine learning classifiers for modeling travel mode choice},
	volume = {78},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417417300738},
	doi = {10.1016/j.eswa.2017.01.057},
	abstract = {The analysis of travel mode choice is an important task in transportation planning and policy making in order to understand and predict travel demands. While advances in machine learning have led to numerous powerful classifiers, their usefulness for modeling travel mode choice remains largely unexplored. Using extensive Dutch travel diary data from the years 2010 to 2012, enriched with variables on the built and natural environment as well as on weather conditions, this study compares the predictive performance of seven selected machine learning classifiers for travel mode choice analysis and makes recommendations for model selection. In addition, it addresses the importance of different variables and how they relate to different travel modes. The results show that random forest performs significantly better than any other of the investigated classifiers, including the commonly used multinomial logit model. While trip distance is found to be the most important variable, the importance of the other variables varies with classifiers and travel modes. The importance of the meteorological variables is highest for support vector machine, while temperature is particularly important for predicting bicycle and public transport trips. The results suggest that the analysis of variable importance with respect to the different classifiers and travel modes is essential for a better understanding and effective modeling of people’s travel behavior.},
	number = {Supplement C},
	urldate = {2017-11-07},
	journal = {Expert Systems with Applications},
	author = {Hagenauer, Julian and Helbich, Marco},
	month = jul,
	year = {2017},
	keywords = {Thesis, Classification, Machine learning, The Netherlands, Travel mode choice},
	pages = {273--282},
	file = {ScienceDirect Full Text PDF:/home/gayouf/Zotero/storage/3NGN3VTE/Hagenauer and Helbich - 2017 - A comparative study of machine learning classifier.pdf:application/pdf;ScienceDirect Snapshot:/home/gayouf/Zotero/storage/S9ENGXDS/S0957417417300738.html:text/html}
}

@article{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2017-11-08},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.04747},
	keywords = {Thesis, Computer Science - Learning},
	file = {arXiv\:1609.04747 PDF:/home/gayouf/Zotero/storage/4F6VG5BF/Ruder - 2016 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/HFBZL8D8/1609.html:text/html}
}

@inproceedings{acuna-agost_airline_2017,
	title = {Airline {Itinerary} {Choice} {Modelling} {Using} {Machine} {Learning}},
	copyright = {Papers presented at the Conference of Choice Modelling are licensed under a Creative Commons Attribution-Non-Commercial 2.0 UK: England \&amp; Wales License ( http://creativecommons.org/licenses/by-nc/2.0/uk/ ).  By submitting a paper,  authors will agree to have their work made available on the conference website.   The authors retain the right to:   - use the paper on personal or institutional websites  - distribute their paper via e-mail or other means, including the version hosted on the conference website  - publish the paper in a journal or book after the end of the conference},
	url = {http://www.icmconference.org.uk/index.php/icmc/ICMC2017/paper/view/1178},
	abstract = {This paper deals with the airline itinerary choice problem. Consider for example that a customer is searching for flights from London to New York, traveling next week on Tuesday and coming back on Saturday. This search request is then processed by a travel provider (e.g., an online travel agent) that proposes between 50 and 100 different alternatives (itineraries) to the customer. The itineraries have different attributes, among others: number of stops, total trip duration, and price. The question is: “which one is (probably) going to be selected by the customer?”   There is a growing interest within the travel industry in better understanding how customers choose between different itineraries when searching for flights. Such an understanding can help travel providers, either airlines or travel agents, to better adapt their offer to market conditions and customer needs, thus increasing their revenue. This can be used for filtering alternatives, sorting them or even for changing some attributes in real-time (e.g., change the price).   The field of customer choice modelling is dominated by traditional statistical approaches, such as the Multinomial Logit (MNL) model, that are linear with respect to features and are tightly bound to their assumptions about the distribution of error (Gaussian, Gumbel, etc.). While these models offer the dual advantages of simplicity and readability, they lack flexibility to handle correlations between alternatives or non-linearity in the effect of alternative attributes. A large part of the existing modelling work focuses in adapting these modelled distributions, so that they can match observed behaviour. Nested (NL) and Cross-Nested (CNL) Logit models are good examples of this: they add terms for highly specific feature interactions, so that substitution patterns between sub-groups of alternatives can be captured.   In this work, we present an alternative modelling approach based on machine learning techniques.  The selected machine learning methods do not require any assumption about the distribution of errors, they can also create non-linear relationships between feature values and the target class, include collinear features, and have more modelling flexibility in general.   In particular, we have chosen to work with Random Forests. Random Forests are ensembles of decision trees which aggregate their predictions. A decision tree is a tree of nodes, each of which applies a linear threshold to a single variable. Each decision tree in a random forest receives a random subset of the input features, and then generates a tree deterministically from those features.   In fact, Random Forests are well adapted for our problem as model bifurcations (branches) automatically partition the customers into segments and, at the same time, it captures nonlinearities relationships within attributes of alternatives and characteristics of the decision maker. Indeed, there are two main segments to be taken into account for our particular problem: business and leisure air passengers behave very different when it comes to book flights. Business passenger tends to favour alternatives with convenient schedules like shorter connection times and time preferences, while leisure passengers are very price sensitive, in other words they can accept a longer connection time if this is reflected in a lower price of the tickets. The problem is that the “segment” is not explicitly known when the customer is searching, however it could be derived by combining different factors. For example, industry experts know that business passengers have a tendency to book with less anticipation and are not predisposed to stay on Saturdays nights. In spite of this, these are not “black or white” rules, so this reinforce the need for a model able to detect these rules depending on the data and actual customer behaviour.   Another observed advantage is that Random Forests are also fairly quick to train and very quick to predict, which enables fast iteration. As a matter of fact, our numerical experiments show that Machine Learning methods were faster than MNL for the learning process, both using public libraries with default parameters.   We trained and tested our models on a dataset consisting of flight searches and bookings on six European markets, extracted from GDS (global distribution system) logs. The choice set consists of the results of a flight search request. Each search request includes between 10 and 250 different itineraries, one of which has been booked by the customer. Choice sets are regrouped by Origin and Destination market.   Our main experiments consist in comparing MNL vs Machine Learning. We evaluate the performance of the different models by comparing their prediction of fractional shares of choices against the actual distribution of choices. It should be noted that in our data set the alternatives are not fixed from one shopping session (choice situation) to the next, we compare the predicted and actual market share of groups of alternatives, such as flight numbers, flights from one airline, or flights departing during a specific time window. These KPIs are particularly useful, as they are often the final result expected from the model.   Our general finding is that, on most origin and destination markets, machine learning models outperform MNL on all relevant metrics. In particular, we found a reduction in the error of airline prediction of more than 70\% on four of the six markets we considered. We also find that training a model on several markets at once results in similar performance – something which could greatly help in scaling the models to a large amount of markets.   The main conclusion of our work is thus that non-linear machine learning methods such as the ones we present here can provide clear benefits to some choice modelling applications, such as air travel itinerary choice by passengers, notably thanks to the better handling of non-linearity, overall greater flexibility they provide, and fast learning computation.},
	language = {en},
	urldate = {2018-01-09},
	booktitle = {International {Choice} {Modelling} {Conference} 2017},
	author = {Acuna-Agost, Rodrigo and Delahaye, Thierry and Lheritier, Alix and Bocamazo, Michael},
	month = mar,
	year = {2017},
	keywords = {Thesis},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/VDTMYMCI/Acuna-Agost et al. - 2017 - Airline Itinerary Choice Modelling Using Machine L.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/EMXDGLRR/1178.html:text/html}
}

@article{hensher_comparison_2000,
	title = {A comparison of the predictive potential of artificial neural networks and nested logit models for commuter mode choice},
	volume = {36},
	issn = {1366-5545},
	url = {http://www.sciencedirect.com/science/article/pii/S1366554599000307},
	doi = {10.1016/S1366-5545(99)00030-7},
	abstract = {Research in the field of artificial intelligence systems has been exploring the use of artificial neural networks (ANN) as a framework within which many traffic and transport problems can be studied. One appeal of ANN is their use of pattern association and error correction to represent a problem. This contrasts with the random utility maximisation rule in discrete choice modelling. ANN enables a full set of human perceptions about a particular problem to be represented by artificial networks of neurons. A claim of ANN is that it can tackle the problem of travel demand forecasting and modelling as well if not better than the discrete choice approach. The use of such tools in studying individual traveller behaviour thus opens up an opportunity to consider the extent to which there are representation frameworks which complement or replace discrete choice methods. This paper explores the merits of neural networks by comparing the predictive capability of ANN and nested logit models in the context of commuter mode choice.},
	number = {3},
	urldate = {2018-02-02},
	journal = {Transportation Research Part E: Logistics and Transportation Review},
	author = {Hensher, David A. and Ton, Tu T.},
	month = sep,
	year = {2000},
	pages = {155--172},
	file = {ScienceDirect Full Text PDF:/home/gayouf/Zotero/storage/X975DIZS/Hensher and Ton - 2000 - A comparison of the predictive potential of artifi.pdf:application/pdf;ScienceDirect Snapshot:/home/gayouf/Zotero/storage/WMFUHG7F/S1366554599000307.html:text/html}
}

@article{karlaftis_statistical_2011,
	title = {Statistical methods versus neural networks in transportation research: {Differences}, similarities and some insights},
	volume = {19},
	issn = {0968-090X},
	shorttitle = {Statistical methods versus neural networks in transportation research},
	url = {http://www.sciencedirect.com/science/article/pii/S0968090X10001610},
	doi = {10.1016/j.trc.2010.10.004},
	abstract = {In the field of transportation, data analysis is probably the most important and widely used research tool available. In the data analysis universe, there are two ‘schools of thought’; the first uses statistics as the tool of choice, while the second – one of the many methods from – Computational Intelligence. Although the goal of both approaches is the same, the two have kept each other at arm’s length. Researchers frequently fail to communicate and even understand each other’s work. In this paper, we discuss differences and similarities between these two approaches, we review relevant literature and attempt to provide a set of insights for selecting the appropriate approach.},
	number = {3},
	urldate = {2018-02-02},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Karlaftis, M. G. and Vlahogianni, E. I.},
	month = jun,
	year = {2011},
	keywords = {Neural networks, Statistical models, Transportation research},
	pages = {387--399},
	file = {ScienceDirect Full Text PDF:/home/gayouf/Zotero/storage/CEW7SSDZ/Karlaftis and Vlahogianni - 2011 - Statistical methods versus neural networks in tran.pdf:application/pdf;ScienceDirect Snapshot:/home/gayouf/Zotero/storage/FFUF3R69/S0968090X10001610.html:text/html}
}

@inproceedings{li_comparison_2010,
	title = {A {Comparison} of {Cross}-{Nested} {Logit} {Model} and {BP} {Neural} {Network} to {Estimate} {Residential} {Location} and {Commute} {Mode} {Choice} in {Beijing}},
	volume = {1},
	doi = {10.1109/ICMTMA.2010.426},
	abstract = {The objective of this paper is to compare the merits of back propagation neural network (BPNN) with those of cross-nested logit (CNL) model to estimate the simultaneously joint choice of residential location and commute mode choice during the process of employment surburbanization. Back propagation neural network and discrete choice model specified as cross-nested logit have been respectively employed to investigate the joint choice for different types of employment destination scenarios, that is, under center (CBD), urban and suburban workplace patterns in Beijing. The predictive capability of these two models has been compared in terms of models accuracy. Results demonstrate that on the whole the BPNN have a higher accuracy for this joint choice and is more suitable for prediction.},
	booktitle = {2010 {International} {Conference} on {Measuring} {Technology} and {Mechatronics} {Automation}},
	author = {Li, X. and Shao, C. and Yang, L. and Ma, Z.},
	month = mar,
	year = {2010},
	keywords = {Predictive models, transportation, Neural networks, Automation, back propagation neural network, backpropagation, Beijing, BPNN, commute mode choice, Commute Mode Choice, Cross-nested Logit, cross-nested logit model, Decision making, discrete choice model, employment, Employment, employment surburbanization, Laboratories, Mathematical model, Mechatronics, neural nets, Residential Location Choice, residential location estimation, social sciences, Spatial Correlation, Transportation, Urban planning, workplace patterns},
	pages = {36--39},
	file = {IEEE Xplore Abstract Record:/home/gayouf/Zotero/storage/RWD2G634/5459104.html:text/html}
}

@inproceedings{barthelemy_robustness_2017,
	title = {Robustness of artificial neural network and discrete choice modelling in presence of unobserved variables},
	abstract = {Models are used to gain a better understanding of complex systems such as the evolution of a population, the transportation demand, the brain behaviour, elections outcome, the propagation of a disease,... System models should be precise and parsimonious. However, the total variation of the system cannot be precisely captured by the observed variables as there can be unobserved ones influencing the system output. The unexplained variation caused by unobserved variables is, therefore, considered as a noise in the model. Different models handle that noise in a different way. For instance, a linear regression assumes that the noise follows a normal distribution and explicitly incorporates it into the model formulation. On the other hand, other models, such as a deterministic neural network, do not explicitly incorporate that noise. Several models can then be applied and the selection of the best one can be a challenging question. This research aims to highlight the importance of the unobserved variables on the results of two types of simple yet widely used models: feedforward neural networks (FFNN) and logit discrete choice models (LDCM). The first application consists in modelling the divorces in an agent-based microsimulation, the agents being the individuals of a given population. For each couple in the model, the divorce is predicted based on the characteristics of the couple (ex: length of the marriage, age of the individuals). In this application, it is shown that the LDCM outperforms the neural network due to the presence of-possibly many-unobserved variables. The second example is a model defined to predict the level of interaction between groundwater and quarry extensions. In this application, the value of every relevant variable is assumed to be known, i.e. the noise from unobserved variables is minimum. In this case, it is shown that both approaches perform well, but FFNN perform slightly better than LDCM. We then investigate how the model performance evolves when the noise increases by removing variables from the models specification. Finally, those two applications will allow us to conclude on the robustness of the discrete choice models and artificial neural network in presence of unobserved variables.},
	author = {Barthélemy, Johan and Dumont, Morgane and Carletti, Timoteo},
	month = dec,
	year = {2017},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/5HWM5V46/Barthélemy et al. - 2017 - Robustness of artificial neural network and discre.pdf:application/pdf}
}

@article{yang_semi-supervised_2017,
	title = {Semi-supervised {Learning} for {Discrete} {Choice} {Models}},
	url = {http://arxiv.org/abs/1702.05137},
	abstract = {We introduce a semi-supervised discrete choice model to calibrate discrete choice models when relatively few requests have both choice sets and stated preferences but the majority only have the choice sets. Two classic semi-supervised learning algorithms, the expectation maximization algorithm and the cluster-and-label algorithm, have been adapted to our choice modeling problem setting. We also develop two new algorithms based on the cluster-and-label algorithm. The new algorithms use the Bayesian Information Criterion to evaluate a clustering setting to automatically adjust the number of clusters. Two computational studies including a hotel booking case and a large-scale airline itinerary shopping case are presented to evaluate the prediction accuracy and computational effort of the proposed algorithms. Algorithmic recommendations are rendered under various scenarios.},
	urldate = {2018-02-05},
	journal = {arXiv:1702.05137 [cs, stat]},
	author = {Yang, Jie and Shebalov, Sergey and Klabjan, Diego},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.05137},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1702.05137 PDF:/home/gayouf/Zotero/storage/NGNYAN8Y/Yang et al. - 2017 - Semi-supervised Learning for Discrete Choice Model.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/GU6K77IX/1702.html:text/html}
}

@article{iranitalab_comparison_2017,
	title = {Comparison of four statistical and machine learning methods for crash severity prediction},
	volume = {108},
	issn = {0001-4575},
	url = {http://www.sciencedirect.com/science/article/pii/S0001457517302865},
	doi = {10.1016/j.aap.2017.08.008},
	abstract = {Crash severity prediction models enable different agencies to predict the severity of a reported crash with unknown severity or the severity of crashes that may be expected to occur sometime in the future. This paper had three main objectives: comparison of the performance of four statistical and machine learning methods including Multinomial Logit (MNL), Nearest Neighbor Classification (NNC), Support Vector Machines (SVM) and Random Forests (RF), in predicting traffic crash severity; developing a crash costs-based approach for comparison of crash severity prediction methods; and investigating the effects of data clustering methods comprising K-means Clustering (KC) and Latent Class Clustering (LCC), on the performance of crash severity prediction models. The 2012–2015 reported crash data from Nebraska, United States was obtained and two-vehicle crashes were extracted as the analysis data. The dataset was split into training/estimation (2012–2014) and validation (2015) subsets. The four prediction methods were trained/estimated using the training/estimation dataset and the correct prediction rates for each crash severity level, overall correct prediction rate and a proposed crash costs-based accuracy measure were obtained for the validation dataset. The correct prediction rates and the proposed approach showed NNC had the best prediction performance in overall and in more severe crashes. RF and SVM had the next two sufficient performances and MNL was the weakest method. Data clustering did not affect the prediction results of SVM, but KC improved the prediction performance of MNL, NNC and RF, while LCC caused improvement in MNL and RF but weakened the performance of NNC. Overall correct prediction rate had almost the exact opposite results compared to the proposed approach, showing that neglecting the crash costs can lead to misjudgment in choosing the right prediction method.},
	urldate = {2018-02-05},
	journal = {Accident Analysis \& Prevention},
	author = {Iranitalab, Amirfarrokh and Khattak, Aemal},
	month = nov,
	year = {2017},
	keywords = {Support vector machines, Crash costs, Multinomial logit, Nearest neighbor classification, Random forests, Traffic crash severity prediction},
	pages = {27--36},
	file = {ScienceDirect Full Text PDF:/home/gayouf/Zotero/storage/YQ7JV726/Iranitalab and Khattak - 2017 - Comparison of four statistical and machine learnin.pdf:application/pdf;ScienceDirect Snapshot:/home/gayouf/Zotero/storage/2QPSDLT9/S0001457517302865.html:text/html}
}

@inproceedings{nam_model_2017,
	title = {A {Model} {Based} on {Deep} {Learning} for {Predicting} {Travel} {Mode} {Choice}},
	abstract = {Recognizing the limitations of previous travel mode choice models such as random utility models and multi-layer perceptron neural network models, this study develops a framework using a deep neural network with deep learning schemes, to predict travelers’ mode choice behavior. Deep neural networks and deep learning are relatively newer models, applied mostly so far to pattern recognition and image/voice processing, and for big data analytics. The authors develop such a choice model with a structure that is appropriate for the travel mode choice problem, and demonstrate the success of the model using an available dataset. The research also develops an important component of the model that takes into account the inherent characteristics of choice models that all individuals have different choice alternatives, an aspect not considered in the neural network models of the past that led to poorer performance. The proposed model is compared against existing mode choice models. The results prove that the new model clearly outperforms the previous mode choice models.},
	author = {Nam, Daisik and Kim, Hyunmyung and Cho, Jaewoo and Jayakrishnan, R},
	month = jan,
	year = {2017},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/P976H2PS/Nam et al. - 2017 - A Model Based on Deep Learning for Predicting Trav.pdf:application/pdf}
}

@inproceedings{hess_can_2017,
	title = {Can a better model specification avoid the need to move away from random utility maximisation?},
	url = {https://trid.trb.org/view/1439171},
	urldate = {2018-02-05},
	author = {Hess, Stephane and Beck, Matthew and Crastes dit Sourd, Romain},
	year = {2017},
	file = {Snapshot:/home/gayouf/Zotero/storage/GM876ZLV/1439171.html:text/html}
}

@article{hess_revisiting_2018,
	title = {Revisiting consistency with random utility maximisation: theory and implications for practical work},
	issn = {0040-5833, 1573-7187},
	shorttitle = {Revisiting consistency with random utility maximisation},
	url = {https://link.springer.com/article/10.1007/s11238-017-9651-7},
	doi = {10.1007/s11238-017-9651-7},
	abstract = {While the paradigm of utility maximisation has formed the basis of the majority of applications in discrete choice modelling for over 40 years, its core assumptions have been questioned by work in both behavioural economics and mathematical psychology as well as more recently by developments in the RUM-oriented choice modelling community. This paper reviews the basic properties with a view to explaining the historical pre-eminence of utility maximisation and addresses the question of what departures from the paradigm may be necessary or wise in order to accommodate richer behavioural patterns. We find that many, though not all, of the behavioural traits discussed in the literature can be approximated sufficiently closely by a random utility framework, allowing analysts to retain the many advantages that such an approach possesses.},
	language = {en},
	urldate = {2018-02-05},
	journal = {Theory and Decision},
	author = {Hess, Stephane and Daly, Andrew and Batley, Richard},
	month = jan,
	year = {2018},
	pages = {1--24},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/2IZMWAQN/Hess et al. - 2018 - Revisiting consistency with random utility maximis.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/ZGY6VB5R/s11238-017-9651-7.html:text/html}
}

@article{tribby_analyzing_2017,
	title = {Analyzing walking route choice through built environments using random forests and discrete choice techniques},
	volume = {44},
	issn = {2399-8083},
	url = {https://doi.org/10.1177/0265813516659286},
	doi = {10.1177/0265813516659286},
	abstract = {Walking is a form of active transportation with numerous benefits, including better health outcomes, lower environmental impacts and stronger communities. Understanding built environmental associations with walking behavior is a key step towards identifying design features that support walking. Human mobility data available through GPS receivers and cell phones, combined with high resolution walkability data, provide a rich source of georeferenced data for analyzing environmental associations with walking behavior. However, traditional techniques such as route choice models have difficulty with highly dimensioned data. This paper develops a novel combination of a data-driven technique with route choice modeling for leveraging walkability audits. Using data from a study in Salt Lake City, UT, USA, we apply the data-driven technique of random forests to select variables for use in walking route choice models. We estimate data-driven route choice models and theory-driven models based on predefined walkability dimensions. Results indicate that the random forest technique selects variables that dramatically improve goodness of fit of walking route choice models relative to models based on predefined walkability dimensions. We compare the theory-driven and data-driven walking route choice models based on interpretability and policy relevance.},
	language = {en},
	number = {6},
	urldate = {2018-02-06},
	journal = {Environment and Planning B: Urban Analytics and City Science},
	author = {Tribby, Calvin P. and Miller, Harvey J. and Brown, Barbara B. and Werner, Carol M. and Smith, Ken R.},
	month = nov,
	year = {2017},
	pages = {1145--1167},
	file = {SAGE PDF Full Text:/home/gayouf/Zotero/storage/D6H5LT73/Tribby et al. - 2017 - Analyzing walking route choice through built envir.pdf:application/pdf}
}

@article{wang_data-driven_2015,
	title = {A {Data}-{Driven} {Network} {Analysis} {Approach} to {Predicting} {Customer} {Choice} {Sets} for {Choice} {Modeling} in {Engineering} {Design}},
	volume = {137},
	issn = {1050-0472},
	url = {http://dx.doi.org/10.1115/1.4030160},
	doi = {10.1115/1.4030160},
	abstract = {In this paper, we propose a data-driven network analysis based approach to predict individual choice sets for customer choice modeling in engineering design. We apply data analytics to mine existing data of customer choice sets, which is then used to predict choice sets for individual customers in a new choice modeling scenario where choice set information is lacking. Product association network is constructed to identify product communities based on existing data of customer choice sets, where links between products reflect the proximity or similarity of two products in customers' perceptual space. To account for customer heterogeneity, customers are classified into clusters (segments) based on their profile attributes and for each cluster the product consideration frequency is computed. For predicting choice sets in a new choice modeling scenario, a probabilistic sampling approach is proposed to integrate product associations, customer segments, and the link strengths in the product association network. In case studies, we first implement the approach using an example with simulated choice set data. The quality of predicted choice sets is examined by assessing the estimation bias of the developed choice model. We then demonstrate the proposed approach using actual survey data of vehicle choice, illustrating the benefits of improving a choice model through choice set prediction and the potential of using such choice models to support engineering design decisions. This research also highlights the benefits and potentials of using network techniques for understanding customer preferences in product design.},
	number = {7},
	urldate = {2018-02-06},
	journal = {Journal of Mechanical Design},
	author = {Wang, Mingxian and Chen, Wei},
	month = jul,
	year = {2015},
	pages = {071410--071410--11},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/WHF7T492/Wang and Chen - 2015 - A Data-Driven Network Analysis Approach to Predict.pdf:application/pdf}
}

@article{ramanujam_data-driven_2015,
	title = {Data-{Driven} {Modeling} of the {Airport} {Configuration} {Selection} {Process}},
	volume = {45},
	issn = {2168-2291},
	doi = {10.1109/THMS.2015.2411743},
	abstract = {The runway configuration is the set of the runways at an airport that are used for arrivals and departures at any time. While many factors, including weather, expected demand, environmental considerations, and coordination of flows with neighboring airports, influence the choice of runway configuration, the actual selection decision is made by air traffic controllers in the airport tower. As a result, the capacity of an airport at any time is dependent on the behavior of human decision makers. This paper develops a statistical model to characterize the configuration selection decision process using empirical observations. The proposed approach, based on the discrete-choice modeling framework, identifies the influence of various factors in terms of the utility function of the decision maker. The parameters of the utility functions are estimated through likelihood maximization. Correlations between different alternatives are captured using a multinomial “nested logit” model. A key novelty of this study is the quantitative assessment of the effect of inertia, or the resistance to configuration changes, on the configuration selection process. The developed models are used to predict the runway configuration 3 h ahead of time, given operating conditions such as wind, visibility, and demand. Case studies based on data from Newark (EWR) and LaGuardia (LGA) airports show that the proposed model predicts runway configuration choices significantly better than a baseline model that only considers the historical frequencies of occurrence of different configurations.},
	number = {4},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Ramanujam, V. and Balakrishnan, H.},
	month = aug,
	year = {2015},
	keywords = {Data models, Predictive models, air traffic control, air traffic controllers, air transportation, airport configuration selection process, airport runway configuration, Airport runway configuration, airport tower, airports, Airports, Atmospheric modeling, configuration selection decision process, data-driven modeling, decision maker utility function, decision making, decision processes, discrete-choice modeling framework, discrete-choice models, empirical observation, human decision makers, LaGuardia airport, maximum-likelihood estimation, multinomial nested logit model, Newark airport, Noise reduction, statistical model, Wind},
	pages = {490--499},
	file = {IEEE Xplore Abstract Record:/home/gayouf/Zotero/storage/ZQZREAD3/7076595.html:text/html;IEEE Xplore Full Text PDF:/home/gayouf/Zotero/storage/4ENPZU4Z/Ramanujam and Balakrishnan - 2015 - Data-Driven Modeling of the Airport Configuration .pdf:application/pdf}
}

@article{brathwaite_machine_2017,
	title = {Machine {Learning} {Meets} {Microeconomics}: {The} {Case} of {Decision} {Trees} and {Discrete} {Choice}},
	shorttitle = {Machine {Learning} {Meets} {Microeconomics}},
	url = {http://arxiv.org/abs/1711.04826},
	abstract = {We provide a microeconomic framework for decision trees: a popular machine learning method. Specifically, we show how decision trees represent a non-compensatory decision protocol known as disjunctions-of-conjunctions and how this protocol generalizes many of the non-compensatory rules used in the discrete choice literature so far. Additionally, we show how existing decision tree variants address many economic concerns that choice modelers might have. Beyond theoretical interpretations, we contribute to the existing literature of two-stage, semi-compensatory modeling and to the existing decision tree literature. In particular, we formulate the first bayesian model tree, thereby allowing for uncertainty in the estimated non-compensatory rules as well as for context-dependent preference heterogeneity in one's second-stage choice model. Using an application of bicycle mode choice in the San Francisco Bay Area, we estimate our bayesian model tree, and we find that it is over 1,000 times more likely to be closer to the true data-generating process than a multinomial logit model (MNL). Qualitatively, our bayesian model tree automatically finds the effect of bicycle infrastructure investment to be moderated by travel distance, socio-demographics and topography, and our model identifies diminishing returns from bike lane investments. These qualitative differences lead to bayesian model tree forecasts that directly align with the observed bicycle mode shares in regions with abundant bicycle infrastructure such as Davis, CA and the Netherlands. In comparison, MNL's forecasts are overly optimistic.},
	urldate = {2018-02-07},
	journal = {arXiv:1711.04826 [stat]},
	author = {Brathwaite, Timothy and Vij, Akshay and Walker, Joan L.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.04826},
	keywords = {Statistics - Applications, 62P20, 62P25, 62P30},
	file = {arXiv\:1711.04826 PDF:/home/gayouf/Zotero/storage/4QRLAKQL/Brathwaite et al. - 2017 - Machine Learning Meets Microeconomics The Case of.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/KQHD4GG2/1711.html:text/html}
}

@article{bierlaire_acceptance_2001,
	title = {The acceptance of modal innovation: {The} case of {Swissmetro}},
	shorttitle = {The acceptance of modal innovation},
	url = {https://infoscience.epfl.ch/record/117140},
	abstract = {Bierlaire, Michel; Axhausen, Kay; Abay, Georg},
	language = {en},
	urldate = {2018-02-26},
	journal = {Swiss Transport Research Conference 2001},
	author = {Bierlaire, Michel and Axhausen, Kay and Abay, Georg},
	month = mar,
	year = {2001},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/RIKIZICK/Bierlaire et al. - The acceptance of modal innovation The case of Sw.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/EQZHWEG3/117140.pdf:application/pdf}
}

@article{bierlaire_biogeme:_2003,
	title = {{BIOGEME}: a free package for the estimation of discrete choice models},
	shorttitle = {{BIOGEME}},
	url = {https://infoscience.epfl.ch/record/117133},
	abstract = {Bierlaire, Michel},
	language = {en},
	urldate = {2018-02-26},
	journal = {Swiss Transport Research Conference 2003},
	author = {Bierlaire, Michel},
	month = mar,
	year = {2003},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/IXCRQKBE/Bierlaire - BIOGEME a free package for the estimation of disc.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/4W65WNNV/117133.pdf:application/pdf}
}

@article{bordes_erratum:_2010,
	title = {Erratum: {SGDQN} is {Less} {Careful} than {Expected}},
	volume = {11},
	issn = {ISSN 1533-7928},
	shorttitle = {Erratum},
	url = {http://www.jmlr.org/papers/v11/bordes10a.html},
	number = {Aug},
	urldate = {2018-03-09},
	journal = {Journal of Machine Learning Research},
	author = {Bordes, Antoine and Bottou, Léon and Gallinari, Patrick and Chang, Jonathan and Smith, S. Alex},
	year = {2010},
	pages = {2229--2240},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/BY9NE82A/Bordes et al. - 2010 - Erratum SGDQN is Less Careful than Expected.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/RT6VSTAE/bordes10a.html:text/html}
}

@incollection{defazio_saga:_2014,
	title = {{SAGA}: {A} {Fast} {Incremental} {Gradient} {Method} {With} {Support} for {Non}-{Strongly} {Convex} {Composite} {Objectives}},
	shorttitle = {{SAGA}},
	url = {http://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf},
	urldate = {2018-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1646--1654},
	file = {NIPS Full Text PDF:/home/gayouf/Zotero/storage/9UJUUQGP/Defazio et al. - 2014 - SAGA A Fast Incremental Gradient Method With Supp.pdf:application/pdf;NIPS Snapshort:/home/gayouf/Zotero/storage/AYAZCIIY/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-obj.html:text/html}
}

@article{byrd_use_2011,
	title = {On the {Use} of {Stochastic} {Hessian} {Information} in {Optimization} {Methods} for {Machine} {Learning}},
	volume = {21},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/abs/10.1137/10079923X},
	doi = {10.1137/10079923X},
	abstract = {This paper describes how to incorporate sampled curvature information in a Newton-CG method and in a limited memory quasi-Newton method for statistical learning. The motivation for this work stems from supervised machine learning applications involving a very large number of training points. We follow a batch approach, also known in the stochastic optimization literature as a sample average approximation approach. Curvature information is incorporated in two subsampled Hessian algorithms, one based on a matrix-free inexact Newton iteration and one on a preconditioned limited memory BFGS iteration. A crucial feature of our technique is that Hessian-vector multiplications are carried out with a significantly smaller sample size than is used for the function and gradient. The efficiency of the proposed methods is illustrated using a machine learning application involving speech recognition.},
	number = {3},
	urldate = {2018-03-09},
	journal = {SIAM Journal on Optimization},
	author = {Byrd, R. and Chin, G. and Neveitt, W. and Nocedal, J.},
	month = jul,
	year = {2011},
	pages = {977--995},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/H9WYE9UI/Byrd et al. - 2011 - On the Use of Stochastic Hessian Information in Op.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/V99C54Q7/10079923X.html:text/html}
}

@article{kiros_training_2013,
	title = {Training {Neural} {Networks} with {Stochastic} {Hessian}-{Free} {Optimization}},
	url = {http://arxiv.org/abs/1301.3641},
	abstract = {Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size. We modify Martens' HF for these settings and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classification and deep autoencoder experiments.},
	urldate = {2018-03-09},
	journal = {arXiv:1301.3641 [cs, stat]},
	author = {Kiros, Ryan},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3641},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1301.3641 PDF:/home/gayouf/Zotero/storage/I4GQXZP5/Kiros - 2013 - Training Neural Networks with Stochastic Hessian-F.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/GTK94PVF/1301.html:text/html}
}

@article{livni_computational_2014,
	title = {On the {Computational} {Efficiency} of {Training} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1410.1141},
	abstract = {It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.},
	urldate = {2018-03-14},
	journal = {arXiv:1410.1141 [cs, stat]},
	author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.1141},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1410.1141 PDF:/home/gayouf/Zotero/storage/ICHHHXPT/Livni et al. - 2014 - On the Computational Efficiency of Training Neural.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/DYMSZGU5/1410.html:text/html}
}

@article{wu_data_2014,
	title = {Data mining with big data},
	volume = {26},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2013.109},
	abstract = {Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wu, X. and Zhu, X. and Wu, G. Q. and Ding, W.},
	month = jan,
	year = {2014},
	keywords = {Data models, autonomous sources, Big Data, Big Data processing model, Big Data revolution, complex and evolving associations, data collection capacity, data driven model, Data handling, data mining, Data privacy, data storage, Data storage systems, demand driven aggregation, Distributed databases, growing data sets, HACE theorem, heterogeneity, Information management, information sources, networking, user interest modeling, user modelling},
	pages = {97--107},
	file = {IEEE Xplore Abstract Record:/home/gayouf/Zotero/storage/SWQJCQGS/6547630.html:text/html}
}

@article{bottou_optimization_2016,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1606.04838},
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	urldate = {2018-03-14},
	journal = {arXiv:1606.04838 [cs, math, stat]},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04838},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv\:1606.04838 PDF:/home/gayouf/Zotero/storage/2TTXQNZJ/Bottou et al. - 2016 - Optimization Methods for Large-Scale Machine Learn.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/6MF6BC4T/1606.html:text/html}
}

@article{duchi_adaptive_2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	number = {Jul},
	urldate = {2018-03-14},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/FVJRA29N/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/NKFVG7TE/duchi11a.html:text/html}
}

@inproceedings{you_investigation_2014,
	title = {Investigation of stochastic {Hessian}-{Free} optimization in {Deep} neural networks for speech recognition},
	doi = {10.1109/ISCSLP.2014.6936597},
	abstract = {Effective training of Deep neural networks (DNNs) has very important significance for the DNNs based speech recognition systems. Stochastic gradient descent (SGD) is the most popular method for training DNNs. SGD often provides the solutions that are well adapt to generalization on held-out data. Recently, Hessian Free (HF) optimization have proved another optional algorithm for training DNNs. HF can be used for solving the pathological tasks. Stochastic Hessian Free (SHF) is a variation of HF, which can combine the generalization advantages of stochastic gradient descent (SGD) with second-order information from Hessian Free. This paper focus on investigating the SHF algorithm for DNN training. We conduct this algorithm on 100 hours Mandarin Chinese recorded speech recognition task. The first experiment shows that choosing proper size of gradient and curvature minibatch results in less training time and good performance. Next, it is observed that the performance of SHF does not depend on the initial parameters. Further more, experimental results shows that SHF performs with comparable results with SGD but better than traditional HF. Finally, we find that additional performance improvement is obtained with a dropout algorithm.},
	booktitle = {The 9th {International} {Symposium} on {Chinese} {Spoken} {Language} {Processing}},
	author = {You, Z. and Xu, B.},
	month = sep,
	year = {2014},
	keywords = {Neural networks, neural nets, deep neural networks, Deep neural networks, DNN training, Dropout, Error analysis, generalization advantages, gradient methods, Hafnium, Hessian free optimization, HF optimization, Mandarin Chinese recorded speech recognition task, natural language processing, Optimization, optional algorithm, pathological tasks, second-order information, SGD, SHF algorithm, speech recognition, Speech recognition, speech recognition system, stochastic gradient descent, stochastic Hessian free, stochastic Hessian-Free optimization, Stochastic Hessian-Free optimization, Stochastic processes, stochastic programming, Training},
	pages = {450--453},
	file = {IEEE Xplore Abstract Record:/home/gayouf/Zotero/storage/JU2THEFX/6936597.html:text/html;IEEE Xplore Full Text PDF:/home/gayouf/Zotero/storage/ULDQWET6/You and Xu - 2014 - Investigation of stochastic Hessian-Free optimizat.pdf:application/pdf}
}

@article{agarwal_second-order_2016,
	title = {Second-{Order} {Stochastic} {Optimization} for {Machine} {Learning} in {Linear} {Time}},
	url = {http://arxiv.org/abs/1602.03943},
	abstract = {First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data.},
	urldate = {2018-03-21},
	journal = {arXiv:1602.03943 [cs, stat]},
	author = {Agarwal, Naman and Bullins, Brian and Hazan, Elad},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.03943},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1602.03943 PDF:/home/gayouf/Zotero/storage/NXUX7NDN/Agarwal et al. - 2016 - Second-Order Stochastic Optimization for Machine L.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/HMM7S7YE/1602.html:text/html}
}

@article{ye_nestrovs_2017,
	title = {Nestrov's {Acceleration} {For} {Second} {Order} {Method}},
	url = {http://arxiv.org/abs/1705.07171},
	abstract = {Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted much attention due to their low computational cost in each iteration. However, these algorithms might perform poorly especially if it is hard to approximate the Hessian well and efficiently. As far as we know, there is no effective way to handle this problem. In this paper, we resort to Nestrov's acceleration technique to improve the convergence performance of a class of second-order methods called approximate Newton. We give a theoretical analysis that Nestrov's acceleration technique can improve the convergence performance for approximate Newton just like for first-order methods. We accordingly propose an accelerated regularized sub-sampled Newton. Our accelerated algorithm performs much better than the original regularized sub-sampled Newton in experiments, which validates our theory empirically. Besides, the accelerated regularized sub-sampled Newton has good performance comparable to or even better than state-of-art algorithms.},
	urldate = {2018-03-21},
	journal = {arXiv:1705.07171 [cs]},
	author = {Ye, Haishan and Zhang, Zhihua},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07171},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/gayouf/Zotero/storage/VGZW8D9S/1705.html:text/html}
}

@incollection{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
	urldate = {2018-03-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {2951--2959},
	file = {NIPS Full Text PDF:/home/gayouf/Zotero/storage/JICDUNEB/Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf:application/pdf;NIPS Snapshort:/home/gayouf/Zotero/storage/RM5LKKB9/4522-practical-bayesian-optimization-of-machine-learning-algorithms.html:text/html}
}

@article{mutny_stochastic_2016,
	title = {Stochastic {Second}-{Order} {Optimization} via von {Neumann} {Series}},
	url = {http://arxiv.org/abs/1612.04694},
	abstract = {A stochastic iterative algorithm approximating second-order information using von Neumann series is discussed. We present convergence guarantees for strongly-convex and smooth functions. Our analysis is much simpler in contrast to a similar algorithm and its analysis, LISSA. The algorithm is primarily suitable for training large scale linear models, where the number of data points is very large. Two novel analyses, one showing space independent linear convergence, and one showing conditional quadratic convergence are discussed. In numerical experiments, the behavior of the error is similar to the second-order algorithm L-BFGS, and improves the performance of LISSA for quadratic objective function.},
	urldate = {2018-03-21},
	journal = {arXiv:1612.04694 [cs, math]},
	author = {Mutny, Mojmir},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.04694},
	keywords = {Mathematics - Optimization and Control, Computer Science - Numerical Analysis},
	file = {arXiv\:1612.04694 PDF:/home/gayouf/Zotero/storage/VZTQI566/Mutny - 2016 - Stochastic Second-Order Optimization via von Neuma.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/ZY8XYLVF/1612.html:text/html}
}

@article{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {http://arxiv.org/abs/1406.2572},
	abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
	urldate = {2018-03-21},
	journal = {arXiv:1406.2572 [cs, math, stat]},
	author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2572},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv\:1406.2572 PDF:/home/gayouf/Zotero/storage/THRYXK8G/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/6ZZGU3CB/1406.html:text/html}
}

@inproceedings{curtis_self-correcting_2016,
	title = {A {Self}-{Correcting} {Variable}-{Metric} {Algorithm} for {Stochastic} {Optimization}},
	url = {http://proceedings.mlr.press/v48/curtis16.html},
	abstract = {An algorithm for stochastic (convex or nonconvex) optimization is presented. The algorithm is variable-metric in the sense that, in each iteration, the step is computed through the product of a sym...},
	language = {en},
	urldate = {2018-03-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Curtis, Frank},
	month = jun,
	year = {2016},
	pages = {632--641},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/M9G5QAZ6/Curtis - 2016 - A Self-Correcting Variable-Metric Algorithm for St.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/PBPLY57Z/curtis16.html:text/html}
}

@inproceedings{keskar_adaqn:_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{adaQN}: {An} {Adaptive} {Quasi}-{Newton} {Algorithm} for {Training} {RNNs}},
	isbn = {978-3-319-46127-4 978-3-319-46128-1},
	shorttitle = {{adaQN}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-46128-1_1},
	doi = {10.1007/978-3-319-46128-1_1},
	abstract = {Recurrent Neural Networks, or RNNs, are powerful models that achieve exceptional performance on a plethora pattern recognition problems. However, the training of RNNs is a computationally difficult task owing to the well-known “vanishing/exploding” gradient problem. Algorithms proposed for training RNNs either exploit no (or limited) curvature information and have cheap per-iteration complexity, or attempt to gain significant curvature information at the cost of increased per-iteration cost. The former set includes diagonally-scaled first-order methods such as Adagrad and Adam, while the latter consists of second-order algorithms like Hessian-Free Newton and K-FAC. In this paper, we present adaQN, a stochastic quasi-Newton algorithm for training RNNs. Our approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. We present numerical experiments on two language modeling tasks and show that adaQN is competitive with popular RNN training algorithms.},
	language = {en},
	urldate = {2018-03-21},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer, Cham},
	author = {Keskar, Nitish Shirish and Berahas, Albert S.},
	month = sep,
	year = {2016},
	pages = {1--16},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/5FYGZCCR/Keskar and Berahas - 2016 - adaQN An Adaptive Quasi-Newton Algorithm for Trai.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/LESMTNNL/978-3-319-46128-1_1.html:text/html}
}

@article{wang_stochastic_2014,
	title = {Stochastic {Quasi}-{Newton} {Methods} for {Nonconvex} {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.1196},
	abstract = {In this paper we study stochastic quasi-Newton methods for nonconvex stochastic optimization, where we assume that only stochastic information of the gradients of the objective function is available via a stochastic first-order oracle (SFO). Firstly, we propose a general framework of stochastic quasi-Newton methods for solving nonconvex stochastic optimization. The proposed framework extends the classic quasi-Newton methods working in deterministic settings to stochastic settings, and we prove its almost sure convergence to stationary points. Secondly, we propose a general framework for a class of randomized stochastic quasi-Newton methods, in which the number of iterations conducted by the algorithm is a random variable. The worst-case SFO-calls complexities of this class of methods are analyzed. Thirdly, we present two specific methods that fall into this framework, namely stochastic damped-BFGS method and stochastic cyclic Barzilai-Borwein method. Finally, we report numerical results to demonstrate the efficiency of the proposed methods.},
	urldate = {2018-03-21},
	journal = {arXiv:1412.1196 [math]},
	author = {Wang, Xiao and Ma, Shiqian and Liu, Wei},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.1196},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv\:1412.1196 PDF:/home/gayouf/Zotero/storage/3Z3HUJVR/Wang et al. - 2014 - Stochastic Quasi-Newton Methods for Nonconvex Stoc.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/T5P29CZY/1412.html:text/html}
}

@article{bordes_sgd-qn:_2009,
	title = {{SGD}-{QN}: {Careful} {Quasi}-{Newton} {Stochastic} {Gradient} {Descent}},
	volume = {10},
	issn = {1532-4435},
	shorttitle = {{SGD}-{QN}},
	url = {http://dl.acm.org/citation.cfm?id=1577069.1755842},
	abstract = {The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient descent but requires less iterations to achieve the same accuracy. This algorithm won the "Wild Track" of the first PASCAL Large Scale Learning Challenge (Sonnenburg et al., 2008).},
	urldate = {2018-03-22},
	journal = {J. Mach. Learn. Res.},
	author = {Bordes, Antoine and Bottou, Léon and Gallinari, Patrick},
	month = dec,
	year = {2009},
	pages = {1737--1754},
	file = {ACM Full Text PDF:/home/gayouf/Zotero/storage/7MX2J6KJ/Bordes et al. - 2009 - SGD-QN Careful Quasi-Newton Stochastic Gradient D.pdf:application/pdf}
}

@book{martens_deep_nodate,
	title = {Deep learning via {Hessian}-free optimization},
	abstract = {We develop a 2 nd-order optimization method based on the “Hessian-free ” approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton \& Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn’t limited in applicability to autoencoders, or any specific model class. We also discuss the issue of “pathological curvature ” as a possible explanation for the difficulty of deeplearning and how 2 nd-order optimization, and our method in particular, effectively deals with it. 1.},
	author = {Martens, James},
	file = {Citeseer - Full Text PDF:/home/gayouf/Zotero/storage/3S965C8T/Martens - Deep learning via Hessian-free optimization.pdf:application/pdf;Citeseer - Snapshot:/home/gayouf/Zotero/storage/IVSSTXVS/summary.html:text/html}
}

@article{gower_accelerated_2018,
	title = {Accelerated {Stochastic} {Matrix} {Inversion}: {General} {Theory} and {Speeding} up {BFGS} {Rules} for {Faster} {Second}-{Order} {Optimization}},
	shorttitle = {Accelerated {Stochastic} {Matrix} {Inversion}},
	url = {http://arxiv.org/abs/1802.04079},
	abstract = {We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning. As an application of our general theory, we develop the \{{\textbackslash}em first accelerated (deterministic and stochastic) quasi-Newton updates\}. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models.},
	urldate = {2018-03-26},
	journal = {arXiv:1802.04079 [cs, math]},
	author = {Gower, Robert M. and Hanzely, Filip and Richtárik, Peter and Stich, Sebastian},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.04079},
	keywords = {Mathematics - Optimization and Control, Computer Science - Numerical Analysis},
	file = {arXiv\:1802.04079 PDF:/home/gayouf/Zotero/storage/DBG3328V/Gower et al. - 2018 - Accelerated Stochastic Matrix Inversion General T.pdf:application/pdf;arXiv.org Snapshot:/home/gayouf/Zotero/storage/LH5WQPXH/1802.html:text/html}
}

@article{mokhtari_res:_2014,
	title = {{RES}: {Regularized} {Stochastic} {BFGS} {Algorithm}},
	volume = {62},
	issn = {1053-587X},
	shorttitle = {{RES}},
	doi = {10.1109/TSP.2014.2357775},
	abstract = {RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method, is proposed to solve strongly convex optimization problems with stochastic objectives. The use of stochastic gradient descent algorithms is widespread, but the number of iterations required to approximate optimal arguments can be prohibitive in high dimensional problems. Application of second-order methods, on the other hand, is impracticable because the computation of objective function Hessian inverses incurs excessive computational cost. BFGS modifies gradient descent by introducing a Hessian approximation matrix computed from finite gradient differences. RES utilizes stochastic gradients in lieu of deterministic gradients for both the determination of descent directions and the approximation of the objective function's curvature. Since stochastic gradients can be computed at manageable computational cost, RES is realizable and retains the convergence rate advantages of its deterministic counterparts. Convergence results show that lower and upper bounds on the Hessian eigenvalues of the sample functions are sufficient to guarantee almost sure convergence of a subsequence generated by RES and convergence of the sequence in expectation to optimal arguments. Numerical experiments showcase reductions in convergence time relative to stochastic gradient descent algorithms and non-regularized stochastic versions of BFGS. An application of RES to the implementation of support vector machines is developed.},
	number = {23},
	journal = {IEEE Transactions on Signal Processing},
	author = {Mokhtari, A. and Ribeiro, A.},
	month = dec,
	year = {2014},
	keywords = {Convergence, gradient methods, Optimization, Approximation algorithms, Approximation methods, Broyden-Fletcher-Goldfarb-Shanno quasi-Newton method, convex optimization, convex programming, eigenvalues and eigenfunctions, Eigenvalues and eigenfunctions, finite gradient differences, Hessian approximation matrix, Hessian eigenvalues, Hessian matrices, large-scale optimization, Linear programming, objective function curvature, Quasi-Newton methods, radio networks, regularized stochastic BFGS algorithm, RES, second-order methods, Signal processing algorithms, stochastic gradient descent algorithm, stochastic optimization, stochastic processes, support vector machines},
	pages = {6089--6104},
	file = {IEEE Xplore Abstract Record:/home/gayouf/Zotero/storage/ZCH3DCNE/6899692.html:text/html;IEEE Xplore Full Text PDF:/home/gayouf/Zotero/storage/2LYCI4IY/Mokhtari and Ribeiro - 2014 - RES Regularized Stochastic BFGS Algorithm.pdf:application/pdf}
}

@book{fletcher_practical_1987,
	address = {New York, NY, USA},
	title = {Practical {Methods} of {Optimization}; (2Nd {Ed}.)},
	isbn = {978-0-471-91547-8},
	publisher = {Wiley-Interscience},
	author = {Fletcher, R.},
	year = {1987}
}

@techreport{caswell_treatise_1685,
	title = {A  treatise of algebra, both historical and practical},
	shorttitle = {A  treatise of algebra, both historical and practical},
	abstract = {by John Wallis},
	language = {eng},
	author = {Caswell, John},
	year = {1685},
	doi = {10.3931/e-rara-8842}
}

@article{armijo_minimization_1966,
	title = {Minimization of functions having {Lipschitz} continuous first partial derivatives},
	volume = {16},
	issn = {0030-8730},
	url = {https://msp.org/pjm/1966/16-1/p01.xhtml},
	number = {1},
	urldate = {2018-04-10},
	journal = {Pacific Journal of Mathematics},
	author = {Armijo, Larry},
	month = jan,
	year = {1966},
	pages = {1--3},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/RISGLMQE/Armijo - 1966 - Minimization of functions having Lipschitz continu.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/4VZR36TM/p01.html:application/xhtml+xml}
}

@article{newman_larch:_2016,
	title = {Computational methods for estimating multinomial, nested, and cross-nested logit models that account for semi-aggregate data.},
	url = {https://orbi.uliege.be/handle/2268/201287},
	language = {en},
	urldate = {2018-04-13},
	journal = {Journal of Choice Modelling},
	number = {26},
	pages = {28--40},	
	author = {Newman, Jeffrey and Lurkin, Virginie and Garrow, Laurie},
	year = {2018}
}

@article{sutton_two_1986,
	title = {Two problems with backpropagation and other steepest-descent learning procedures for networks},
	url = {https://ci.nii.ac.jp/naid/10022346408},
	urldate = {2018-04-13},
	journal = {Proceedings of Eightth Annual Conference of the Cognitive Science Society, 1986},
	author = {Sutton, Richard S.},
	year = {1986},
	file = {Two problems with backpropagation and other steepest-descent learning procedures for networks Snapshot:/home/gayouf/Zotero/storage/B6U5DB3H/10022346408.html:text/html}
}

@article{qian_momentum_1999,
	title = {On the momentum term in gradient descent learning algorithms},
	volume = {12},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608098001166},
	doi = {10.1016/S0893-6080(98)00116-6},
	abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
	number = {1},
	urldate = {2018-04-13},
	journal = {Neural Networks},
	author = {Qian, Ning},
	month = jan,
	year = {1999},
	keywords = {Critical damping, Damped harmonic oscillator, Gradient descent learning algorithm, Learning rate, Momentum, Speed of convergence},
	pages = {145--151},
	file = {ScienceDirect Full Text PDF:/home/gayouf/Zotero/storage/QANTY5I7/Qian - 1999 - On the momentum term in gradient descent learning .pdf:application/pdf;ScienceDirect Snapshot:/home/gayouf/Zotero/storage/U5QI3MFY/S0893608098001166.html:text/html}
}

@article{dozat_incorporating_2016,
	title = {Incorporating {Nesterov} {Momentum} into {Adam}},
	url = {https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ},
	abstract = {This work aims to improve upon the recently proposed and rapidly popular-
  ized optimization algorithm Adam (Kingma \& Ba, 2014). Adam has two main
  components—a momentum component and an adaptive...},
	urldate = {2018-04-13},
	author = {Dozat, Timothy},
	month = feb,
	year = {2016},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/ZDTIA38K/Dozat - 2016 - Incorporating Nesterov Momentum into Adam.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/XTLIWW8I/forum.html:text/html}
}

@article{reddi_convergence_2018,
	title = {On the {Convergence} of {Adam} and {Beyond}},
	url = {https://openreview.net/forum?id=ryQu7f-RZ},
	abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates...},
	urldate = {2018-04-13},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/gayouf/Zotero/storage/H634LW8T/Reddi et al. - 2018 - On the Convergence of Adam and Beyond.pdf:application/pdf;Snapshot:/home/gayouf/Zotero/storage/LYWDJT2Q/forum.html:text/html}
}